{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "#TF&K\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, losses, models, callbacks\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.constraints import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "#NLP\n",
    "import bert_tokenization as tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from transformers import TFPreTrainedModel, TFBertMainLayer, BertConfig, TFBertModel, BertTokenizer\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import math\n",
    "import glob\n",
    "import sys\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "from math import floor, ceil\n",
    "\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from os.path import join as path_join\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import KFold, GroupKFold, train_test_split\n",
    "from ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.56)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "SEED = 69\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {\"aren't\" : \"are not\",          \"can't\" : \"cannot\",\n",
    "                \"couldn't\" : \"could not\",      \"couldnt\" : \"could not\",\n",
    "                \"didn't\" : \"did not\",          \"doesn't\" : \"does not\",\n",
    "                \"doesnt\" : \"does not\",         \"don't\" : \"do not\",\n",
    "                \"hadn't\" : \"had not\",          \"hasn't\" : \"has not\",\n",
    "                \"haven't\" : \"have not\",        \"havent\" : \"have not\",\n",
    "                \"he'd\" : \"he would\",           \"he'll\" : \"he will\",\n",
    "                \"he's\" : \"he is\",              \"i'd\" : \"I would\",\n",
    "                \"i'd\" : \"I had\",               \"i'll\" : \"I will\",\n",
    "                \"i'm\" : \"I am\",                \"isn't\" : \"is not\",\n",
    "                \"it's\" : \"it is\",              \"it'll\":\"it will\",\n",
    "                \"i've\" : \"I have\",             \"let's\" : \"let us\",\n",
    "                \"mightn't\" : \"might not\",      \"mustn't\" : \"must not\",\n",
    "                \"shan't\" : \"shall not\",        \"she'd\" : \"she would\",\n",
    "                \"she'll\" : \"she will\",         \"she's\" : \"she is\",\n",
    "                \"shouldn't\" : \"should not\",    \"shouldnt\" : \"should not\",\n",
    "                \"that's\" : \"that is\",          \"thats\" : \"that is\",\n",
    "                \"there's\" : \"there is\",        \"theres\" : \"there is\",\n",
    "                \"they'd\" : \"they would\",       \"they'll\" : \"they will\",\n",
    "                \"they're\" : \"they are\",        \"theyre\":  \"they are\",\n",
    "                \"they've\" : \"they have\",       \"we'd\" : \"we would\",\n",
    "                \"we're\" : \"we are\",            \"weren't\" : \"were not\",\n",
    "                \"we've\" : \"we have\",           \"what'll\" : \"what will\",\n",
    "                \"what're\" : \"what are\",        \"what's\" : \"what is\",\n",
    "                \"what've\" : \"what have\",       \"where's\" : \"where is\",\n",
    "                \"who'd\" : \"who would\",         \"who'll\" : \"who will\",\n",
    "                \"who're\" : \"who are\",          \"who's\" : \"who is\",\n",
    "                \"who've\" : \"who have\",         \"won't\" : \"will not\",\n",
    "                \"wouldn't\" : \"would not\",      \"you'd\" : \"you would\",\n",
    "                \"you'll\" : \"you will\",         \"you're\" : \"you are\",\n",
    "                \"you've\" : \"you have\",         \"'re\": \" are\",\n",
    "                \"wasn't\": \"was not\",           \"we'll\":\" will\",\n",
    "                \"didn't\": \"did not\",           \"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "          '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "          '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "          '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n",
    "          '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑',\n",
    "          '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    "          '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫',\n",
    "          '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    "          '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・',\n",
    "          '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x).replace(\"\\n\",\"\")\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x= clean_text(x.lower())\n",
    "    x= replace_typical_misspell(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../input/google-quest-challenge/'\n",
    "BERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\n",
    "tokenizer2 = BertTokenizer.from_pretrained(BERT_PATH+'/assets/vocab.txt', do_lower_case=True,)\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "df_train = pd.read_csv(PATH+'train.csv')\n",
    "df_test = pd.read_csv(PATH+'test.csv')\n",
    "df = pd.concat([df_train,df_test],axis=0,ignore_index=True)\n",
    "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)\n",
    "print('submission shape =', df_sub.shape)\n",
    "\n",
    "output_categories = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
    "input_categories = ['question_title', 'question_body', 'answer']\n",
    "print('\\noutput categories:\\n\\t', output_categories)\n",
    "print('\\ninput categories:\\n\\t', input_categories)\n",
    "\n",
    "PATH = '../input/google-quest-challenge/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['question_body'] = df_train['question_body'].progress_map(lambda q: preprocess(q))\n",
    "df_train['answer'] = df_train['answer'].progress_map(lambda q: preprocess(q))\n",
    "df_train['question_title'] = df_train['question_title'].progress_map(lambda q: preprocess(q))\n",
    "\n",
    "df_test['question_body'] = df_test['question_body'].progress_map(lambda q: preprocess(q))\n",
    "df_test['answer'] = df_test['answer'].progress_map(lambda q: preprocess(q))\n",
    "df_test['question_title'] = df_test['question_title'].progress_map(lambda q: preprocess(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_masks(tokens, max_seq_length):\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens)+[0]*(max_seq_length-len(tokens))\n",
    "\n",
    "def _get_segments(tokens, max_seq_length):\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    \n",
    "    segments=[]\n",
    "    first_sep=True\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == '[SEP]':\n",
    "            current_segment_id=1\n",
    "    return segments+[0]*(max_seq_length-len(tokens))\n",
    "\n",
    "def _trim_input(title, question, answer, max_sequence_length, t_max_len=30, q_max_len=239, a_max_len=239):\n",
    "\n",
    "    t = tokenizer2.tokenize(title)\n",
    "    q = tokenizer2.tokenize(question)\n",
    "    a = tokenizer2.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n",
    "        \n",
    "        t = t[:t_new_len]\n",
    "        q = q[:q_new_len]\n",
    "        a = a[:a_new_len]\n",
    "    \n",
    "    return t, q, a\n",
    "\n",
    "def _get_ids(tokens, tokenizer, max_seq_length):\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0]*(max_seq_length - len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_seq_length):\n",
    "    stoken = ['[CLS]']+title+['[QBODY]']+question+['[SEP]']+answer+['[SEP]']\n",
    "    #stoken = ['[CLS]']+title+['[QBODY]']+question+['[ANS]']+answer+['[SEP]']\n",
    "    input_ids = _get_ids(tokens=stoken, tokenizer=tokenizer,max_seq_length=max_seq_length)\n",
    "    input_masks = _get_masks(tokens=stoken, max_seq_length=max_seq_length)\n",
    "    input_segments = _get_segments(tokens=stoken, max_seq_length=max_seq_length)\n",
    "    \n",
    "    return [input_ids, input_masks, input_segments]\n",
    "\n",
    "def compute_input_array(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for _, col in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = col.question_title, col.question_body, col.answer\n",
    "        t,q,a = _trim_input(t,q,a, max_sequence_length)\n",
    "        ids, masks, segments = _convert_to_bert_inputs(t,q,a, tokenizer, max_sequence_length)\n",
    "        input_ids.append(ids)\n",
    "        input_masks.append(masks)\n",
    "        input_segments.append(segments)\n",
    "        \n",
    "    return [np.array(input_ids, dtype=np.int32),\n",
    "            np.array(input_masks, dtype=np.int32), \n",
    "            np.array(input_segments, dtype=np.int32)]\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "def compute_spearmanr(trues, preds):\n",
    "    rhos = []\n",
    "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "        rhos.append(\n",
    "            #spearmanr(col_trues, col_pred).correlation)\n",
    "            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
    "    return np.mean(rhos)\n",
    "    #return np.nanmean(rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, valid_data, test_data, test_predictions=test_predictions, batch_size=16, fold=None):\n",
    "        self.valid_inputs = valid_data[0]\n",
    "        self.valid_outputs = valid_data[1]\n",
    "        self.test_inputs = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.test_predictions = test_predictions\n",
    "        self.fold = fold\n",
    "        self.value = -1\n",
    "        self.bad_epochs = 0\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions=[]\n",
    "        #self.test_predictions=[]\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions.append(self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n",
    "        \n",
    "        rho_val = compute_spearmanr(self.valid_outputs, np.average(self.valid_predictions, axis=0))\n",
    "        print(f\"\\nvalidation rho: {round(rho_val,4)}\")\n",
    "        \n",
    "        if rho_val >= self.value:\n",
    "            self.value = rho_val\n",
    "            self.model.save_weights(f'/kaggle/working/bert-base-{fold}.hdf5')\n",
    "        \n",
    "        if (epoch)%4==1 and math.isnan(rho_val):\n",
    "            self.model.save_weights(f'/kaggle/working/bert-base-{fold}.hdf5')\n",
    "            \n",
    "        self.test_predictions.append(self.model.predict(self.test_inputs, batch_size=self.batch_size))\n",
    "        \n",
    "def bce(t,p):\n",
    "    return binary_crossentropy(t,p)\n",
    "\n",
    "def custom_loss(true,pred):\n",
    "    bce = binary_crossentropy(true,pred)\n",
    "    return bce + logcosh(true,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config=BertConfig(unk_token=\"[QBODY]\", pad_token=\"[ANS]\").from_pretrained('../input/bert-tensorflow/bert-base-uncased-config.json',output_hidden_states=False)\n",
    "\n",
    "def bert_model():\n",
    "\n",
    "    input_ids = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_word_ids')\n",
    "    input_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_masks')\n",
    "    input_segments = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH), dtype = tf.int32, name = 'input_segments')\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(pretrained_model_name_or_path='../input/bert-tensorflow/bert-base-uncased-tf_model.h5',config = bert_config)\n",
    "\n",
    "    sequence_output, pooler_output  = bert_model([input_ids,input_mask, input_segments])\n",
    "    \n",
    "    avgpool = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "    x = tf.keras.layers.Dropout(0.2)(avgpool)\n",
    "    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, input_mask, input_segments], outputs=out)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=3e-5),\n",
    "        #loss=['binary_crossentropy']\n",
    "        loss = custom_loss,\n",
    "        metrics = [bce,logcosh]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body)\n",
    "\n",
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_array(df_train, input_categories, tokenizer2, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_array(df_test, input_categories, tokenizer2, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    \n",
    "    if fold in [0,2,4,6,8]:\n",
    "        K.clear_session()\n",
    "        model = bert_model()\n",
    "        train_inputs = [inputs[i][train_idx] for i in range(3)] \n",
    "        train_outputs = outputs[train_idx]\n",
    "    \n",
    "        valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n",
    "        valid_outputs = outputs[valid_idx]\n",
    "    \n",
    "        custom_callback = CustomCallback(valid_data=(valid_inputs,valid_outputs), \n",
    "                                         test_data=test_inputs,\n",
    "                                         batch_size=8, fold=fold)\n",
    "        H = model.fit(train_inputs,train_outputs, batch_size=8, epochs=5, callbacks=[custom_callback])\n",
    "        histories.append(H)\n",
    "        \n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BERT_test_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\n",
    "max_val = BERT_test_preds.max() + 1\n",
    "BERT_test_preds = BERT_test_preds/max_val + 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_test_preds = BERT_test_preds\n",
    "Final_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[targets] = Final_test_preds\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03d22e2121ed46089fa01a8f5eafd6cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e9329dd5c4345498d7d2025194ef525": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "21650f20f11b4afb94c6cf62e91530da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_03d22e2121ed46089fa01a8f5eafd6cb",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fec823980a3a4aecba802110d2a76f41",
       "value": 0
      }
     },
     "2a7fa646373844779d27aa8c312f0e6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6fa8253c59b448209e4964ea356a326f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_21650f20f11b4afb94c6cf62e91530da",
        "IPY_MODEL_86b77d980a1d42479e3823d23b809d07"
       ],
       "layout": "IPY_MODEL_a824db755a64476fa3e287489d993089"
      }
     },
     "86b77d980a1d42479e3823d23b809d07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2a7fa646373844779d27aa8c312f0e6a",
       "placeholder": "​",
       "style": "IPY_MODEL_1e9329dd5c4345498d7d2025194ef525",
       "value": " 0/? [00:00&lt;?, ?it/s]"
      }
     },
     "a824db755a64476fa3e287489d993089": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fec823980a3a4aecba802110d2a76f41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
