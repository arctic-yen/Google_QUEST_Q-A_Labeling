{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as path_join\n",
    "import re\n",
    "import gc\n",
    "import pickle  \n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, models, callbacks\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "import math\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold, GroupKFold, train_test_split\n",
    "from ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.56)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "SEED = 69\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "        \n",
    "def fetch_vectors(string_list, batch_size=64):\n",
    "    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    fin_features = []\n",
    "    for data in chunks(string_list, batch_size):\n",
    "        tokenized = []\n",
    "        for x in data:\n",
    "            x = \" \".join(x.strip().split()[:300])\n",
    "            tok = tokenizer.encode(x, add_special_tokens=True)\n",
    "            tokenized.append(tok[:512])\n",
    "\n",
    "        max_len = 512\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded).to(DEVICE)\n",
    "        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
    "        fin_features.append(features)\n",
    "\n",
    "    fin_features = np.vstack(fin_features)\n",
    "    return fin_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {\"aren't\" : \"are not\",          \"can't\" : \"cannot\",\n",
    "                \"couldn't\" : \"could not\",      \"couldnt\" : \"could not\",\n",
    "                \"didn't\" : \"did not\",          \"doesn't\" : \"does not\",\n",
    "                \"doesnt\" : \"does not\",         \"don't\" : \"do not\",\n",
    "                \"hadn't\" : \"had not\",          \"hasn't\" : \"has not\",\n",
    "                \"haven't\" : \"have not\",        \"havent\" : \"have not\",\n",
    "                \"he'd\" : \"he would\",           \"he'll\" : \"he will\",\n",
    "                \"he's\" : \"he is\",              \"i'd\" : \"I would\",\n",
    "                \"i'd\" : \"I had\",               \"i'll\" : \"I will\",\n",
    "                \"i'm\" : \"I am\",                \"isn't\" : \"is not\",\n",
    "                \"it's\" : \"it is\",              \"it'll\":\"it will\",\n",
    "                \"i've\" : \"I have\",             \"let's\" : \"let us\",\n",
    "                \"mightn't\" : \"might not\",      \"mustn't\" : \"must not\",\n",
    "                \"shan't\" : \"shall not\",        \"she'd\" : \"she would\",\n",
    "                \"she'll\" : \"she will\",         \"she's\" : \"she is\",\n",
    "                \"shouldn't\" : \"should not\",    \"shouldnt\" : \"should not\",\n",
    "                \"that's\" : \"that is\",          \"thats\" : \"that is\",\n",
    "                \"there's\" : \"there is\",        \"theres\" : \"there is\",\n",
    "                \"they'd\" : \"they would\",       \"they'll\" : \"they will\",\n",
    "                \"they're\" : \"they are\",        \"theyre\":  \"they are\",\n",
    "                \"they've\" : \"they have\",       \"we'd\" : \"we would\",\n",
    "                \"we're\" : \"we are\",            \"weren't\" : \"were not\",\n",
    "                \"we've\" : \"we have\",           \"what'll\" : \"what will\",\n",
    "                \"what're\" : \"what are\",        \"what's\" : \"what is\",\n",
    "                \"what've\" : \"what have\",       \"where's\" : \"where is\",\n",
    "                \"who'd\" : \"who would\",         \"who'll\" : \"who will\",\n",
    "                \"who're\" : \"who are\",          \"who's\" : \"who is\",\n",
    "                \"who've\" : \"who have\",         \"won't\" : \"will not\",\n",
    "                \"wouldn't\" : \"would not\",      \"you'd\" : \"you would\",\n",
    "                \"you'll\" : \"you will\",         \"you're\" : \"you are\",\n",
    "                \"you've\" : \"you have\",         \"'re\": \" are\",\n",
    "                \"wasn't\": \"was not\",           \"we'll\":\" will\",\n",
    "                \"didn't\": \"did not\",           \"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "          '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "          '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "          '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n",
    "          '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑',\n",
    "          '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    "          '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫',\n",
    "          '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    "          '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・',\n",
    "          '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x).replace(\"\\n\",\"\")\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x= clean_text(x.lower())\n",
    "    x= replace_typical_misspell(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../input/google-quest-challenge/'\n",
    "df_train = pd.read_csv(path_join(data_path, 'train.csv')).fillna(\"none\")\n",
    "df_test = pd.read_csv(path_join(data_path, 'test.csv')).fillna(\"none\")\n",
    "sample = pd.read_csv(path_join(data_path, 'sample_submission.csv'))\n",
    "target_cols = list(sample.drop(\"qa_id\", axis=1).columns)\n",
    "\n",
    "df_train['question_body'] = df_train['question_body'].progress_map(lambda q: preprocess(q))\n",
    "df_train['answer'] = df_train['answer'].progress_map(lambda q: preprocess(q))\n",
    "df_train['question_title'] = df_train['question_title'].progress_map(lambda q: preprocess(q))\n",
    "#df_train['category'] = df_train['category'].progress_map(lambda q: preprocess(q))\n",
    "\n",
    "df_test['question_body'] = df_test['question_body'].progress_map(lambda q: preprocess(q))\n",
    "df_test['answer'] = df_test['answer'].progress_map(lambda q: preprocess(q))\n",
    "df_test['question_title'] = df_test['question_title'].progress_map(lambda q: preprocess(q))\n",
    "#df_test['category'] = df_test['category'].progress_map(lambda q: preprocess(q))\n",
    "\n",
    "#train_category_dense = fetch_vectors(df_train.category.values)\n",
    "#print('train_category_dense')\n",
    "train_question_title_dense = fetch_vectors(df_train.question_title.values)\n",
    "print('train_question_title_dense')\n",
    "train_question_body_dense = fetch_vectors(df_train.question_body.values)\n",
    "print('train_question_body_dense')\n",
    "train_answer_dense = fetch_vectors(df_train.answer.values)\n",
    "print('train_answer_dense')\n",
    "\n",
    "#test_category_dense = fetch_vectors(df_test.category.values)\n",
    "#print('test_category_dense')\n",
    "test_question_title_dense = fetch_vectors(df_test.question_title.values)\n",
    "print('test_question_title_dense')\n",
    "test_question_body_dense = fetch_vectors(df_test.question_body.values)\n",
    "print('test_question_body_dense')\n",
    "test_answer_dense = fetch_vectors(df_test.answer.values)\n",
    "print('test_answer_dense')\n",
    "\n",
    "del df_train, df_test, sample\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\n",
    "        'question_asker_intent_understanding',\n",
    "        'question_body_critical',\n",
    "        'question_conversational',\n",
    "        'question_expect_short_answer',\n",
    "        'question_fact_seeking',\n",
    "        'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others',\n",
    "        'question_interestingness_self',\n",
    "        'question_multi_intent',\n",
    "        'question_not_really_a_question',\n",
    "        'question_opinion_seeking',\n",
    "        'question_type_choice',\n",
    "        'question_type_compare',\n",
    "        'question_type_consequence',\n",
    "        'question_type_definition',\n",
    "        'question_type_entity',\n",
    "        'question_type_instructions',\n",
    "        'question_type_procedure',\n",
    "        'question_type_reason_explanation',\n",
    "        'question_type_spelling',\n",
    "        'question_well_written',\n",
    "        'answer_helpful',\n",
    "        'answer_level_of_information',\n",
    "        'answer_plausible',\n",
    "        'answer_relevance',\n",
    "        'answer_satisfaction',\n",
    "        'answer_type_instructions',\n",
    "        'answer_type_procedure',\n",
    "        'answer_type_reason_explanation',\n",
    "        'answer_well_written'    \n",
    "    ]\n",
    "\n",
    "input_columns = ['question_title', 'question_body', 'answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find = re.compile(r\"^[^.]*\")\n",
    "\n",
    "df_train['netloc'] = df_train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "df_test['netloc'] = df_test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "\n",
    "features = ['netloc', 'category'] #\n",
    "merged = pd.concat([df_train[features], df_test[features]])\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(merged)\n",
    "\n",
    "features_train = ohe.transform(df_train[features]).toarray()\n",
    "features_test = ohe.transform(df_test[features]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"../input/universal-sentence-encoder-l5/universal_sentence_encoder_large5/\"\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = {}\n",
    "embeddings_test = {}\n",
    "for text in input_columns:\n",
    "    print(text)\n",
    "    train_text = df_train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    test_text = df_test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    \n",
    "    curr_train_emb = []\n",
    "    curr_test_emb = []\n",
    "    batch_size = 4\n",
    "    ind = 0\n",
    "    while ind*batch_size < len(train_text):\n",
    "        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size]).numpy())\n",
    "        ind += 1\n",
    "        \n",
    "    ind = 0\n",
    "    while ind*batch_size < len(test_text):\n",
    "        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size]).numpy())\n",
    "        ind += 1    \n",
    "        \n",
    "    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n",
    "    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n",
    "    \n",
    "del embed\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n",
    "cos_dist = lambda x, y: (x * y).sum(axis=1)\n",
    "abs_dist = lambda x, y: np.abs(x - y).sum(axis=1)\n",
    "sum_dist = lambda x, y: (x + y).sum(axis=1)\n",
    "\n",
    "dist_features_train = np.array([\n",
    "    #l2_dist(embeddings_train['question_title_embedding'], embeddings_train['category_embedding']),\n",
    "    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n",
    "    \n",
    "    #cos_dist(embeddings_train['question_title_embedding'], embeddings_train['category_embedding']),\n",
    "    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n",
    "    \n",
    "    #abs_dist(embeddings_train['question_title_embedding'], embeddings_train['category_embedding']),\n",
    "    abs_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    abs_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    abs_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n",
    "    \n",
    "    #sum_dist(embeddings_train['question_title_embedding'], embeddings_train['category_embedding']),\n",
    "    sum_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    sum_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    sum_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n",
    "]).T\n",
    "\n",
    "dist_features_test = np.array([\n",
    "    #l2_dist(embeddings_test['question_title_embedding'], embeddings_test['category_embedding']),\n",
    "    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n",
    "    \n",
    "    #cos_dist(embeddings_test['question_title_embedding'], embeddings_test['category_embedding']),\n",
    "    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n",
    "    \n",
    "    #abs_dist(embeddings_test['question_title_embedding'], embeddings_test['category_embedding']),\n",
    "    abs_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    abs_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    abs_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n",
    "    \n",
    "    #sum_dist(embeddings_test['question_title_embedding'], embeddings_test['category_embedding']),\n",
    "    sum_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    sum_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    sum_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n",
    "]).T\n",
    "\n",
    "dist_features_train_dense = np.array([\n",
    "    #l2_dist(train_question_title_dense, train_category_dense),\n",
    "    l2_dist(train_question_body_dense, train_answer_dense),\n",
    "    l2_dist(train_question_body_dense, train_question_title_dense),\n",
    "    l2_dist(train_answer_dense, train_question_title_dense),\n",
    "    \n",
    "    #cos_dist(train_question_title_dense, train_category_dense),\n",
    "    cos_dist(train_question_body_dense, train_answer_dense),\n",
    "    cos_dist(train_question_body_dense, train_question_title_dense),\n",
    "    cos_dist(train_answer_dense, train_question_title_dense),\n",
    "    \n",
    "    #abs_dist(train_question_title_dense, train_category_dense),\n",
    "    abs_dist(train_question_body_dense, train_answer_dense),\n",
    "    abs_dist(train_question_body_dense, train_question_title_dense),\n",
    "    abs_dist(train_answer_dense, train_question_title_dense),\n",
    "    \n",
    "    #sum_dist(train_question_title_dense, train_category_dense),\n",
    "    sum_dist(train_question_body_dense, train_answer_dense),  \n",
    "    sum_dist(train_question_body_dense, train_question_title_dense),\n",
    "    sum_dist(train_answer_dense, train_question_title_dense),\n",
    "]).T\n",
    "\n",
    "dist_features_test_dense = np.array([\n",
    "    #l2_dist(test_question_title_dense, test_category_dense),\n",
    "    l2_dist(test_question_body_dense, test_answer_dense),\n",
    "    l2_dist(test_question_body_dense, test_question_title_dense),\n",
    "    l2_dist(test_answer_dense, test_question_title_dense),\n",
    "    \n",
    "    #cos_dist(test_question_title_dense, test_category_dense),\n",
    "    cos_dist(test_question_body_dense, test_answer_dense),\n",
    "    cos_dist(test_question_body_dense, test_question_title_dense),\n",
    "    cos_dist(test_answer_dense, test_question_title_dense),\n",
    "    \n",
    "    #abs_dist(test_question_title_dense, test_category_dense),\n",
    "    abs_dist(test_question_body_dense, test_answer_dense),\n",
    "    abs_dist(test_question_body_dense, test_question_title_dense),\n",
    "    abs_dist(test_answer_dense, test_question_title_dense),\n",
    "    \n",
    "    #sum_dist(test_question_title_dense, test_category_dense),\n",
    "    sum_dist(test_question_body_dense, test_answer_dense),\n",
    "    sum_dist(test_question_body_dense, test_question_title_dense),\n",
    "    sum_dist(test_answer_dense, test_question_title_dense),\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([item for k, item in embeddings_train.items()])\n",
    "X_test = np.hstack([item for k, item in embeddings_test.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, train_question_body_dense, train_answer_dense, train_question_title_dense))\n",
    "X_test = np.hstack((X_test, test_question_body_dense, test_answer_dense, test_question_title_dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train, features_train, dist_features_train, dist_features_train_dense))\n",
    "X_test = np.hstack((X_test, features_test, dist_features_test, dist_features_test_dense))\n",
    "y_train = train[targets].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.RandomUniform(seed=10000)\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        #self.trainable_weights = [self.W]\n",
    "        self.trainable_W = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {'return_attention': self.return_attention}\n",
    "        base_config = super(AttentionWeightedAverage, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpearmanRhoCallback(Callback):\n",
    "    def __init__(self, training_data, validation_data, patience, model_name):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.value = -1\n",
    "        self.bad_epochs = 0\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n",
    "        #rho_val = np.mean([ spearmanr(self.y_val[:, ind], y_pred_val[:, ind]).correlation for ind in range(y_pred_val.shape[1]) ])\n",
    "        if rho_val >= self.value:\n",
    "            self.value = rho_val\n",
    "            self.model.save_weights(self.model_name)\n",
    "        else:\n",
    "            self.bad_epochs += 1\n",
    "        \n",
    "        if self.bad_epochs >= self.patience:\n",
    "            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n",
    "        return rho_val\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "def bce(t,p):\n",
    "    return binary_crossentropy(t,p)\n",
    "\n",
    "def custom_loss(true,pred):\n",
    "    bce = binary_crossentropy(true,pred)\n",
    "    return bce + logcosh(true,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_CNN_model():\n",
    "    \n",
    "    inp = Input(shape=(X_test.shape[1],X_test.shape[2]))\n",
    "    #CNN\n",
    "    conv_0 = Conv1D(filters=256, kernel_size=1, kernel_initializer=lecun_normal(seed=SEED),activation='elu',padding='same')(inp)\n",
    "    maxpool_0 = GlobalMaxPool1D()(conv_0)\n",
    "    conv_1 = Conv1D(filters=256, kernel_size=3, kernel_initializer=lecun_normal(seed=SEED),activation='elu',padding='same')(inp)\n",
    "    maxpool_1 = GlobalMaxPool1D()(conv_1)\n",
    "    conv_2 = Conv1D(filters=256, kernel_size=6, kernel_initializer=lecun_normal(seed=SEED),activation='elu',padding='same')(inp)\n",
    "    maxpool_2 = GlobalMaxPool1D()(conv_2)\n",
    "    conv_3 = Conv1D(filters=256, kernel_size=9, kernel_initializer=lecun_normal(seed=SEED),activation='elu',padding='same')(inp)\n",
    "    maxpool_3 = GlobalMaxPool1D()(conv_3)\n",
    "    concatenate1 = concatenate([maxpool_0, maxpool_1, maxpool_2, maxpool_3],axis=1)\n",
    "    flatten = Flatten()(concatenate1)\n",
    "    x1 = Dense(512, activation='elu', kernel_initializer=lecun_normal(seed=SEED))(concatenate1)\n",
    "    x1 = Dropout(0.2)(x1)\n",
    "    \n",
    "    #RNN\n",
    "    x_gru = Bidirectional(GRU(256, return_sequences = True,\n",
    "                              dropout=0.2,\n",
    "                              recurrent_dropout=0.2,\n",
    "                              kernel_initializer=glorot_uniform(seed=SEED),\n",
    "                              recurrent_initializer=Orthogonal(gain=1.0, seed=SEED)))(inp)\n",
    "    maxpoolGRU = GlobalMaxPooling1D()(x_gru)\n",
    "    avgpoolGRU = GlobalAveragePooling1D()(x_gru)\n",
    "    atnwavgGRU = AttentionWeightedAverage()(x_gru)\n",
    "    concatenate2 = concatenate([atnwavgGRU, avgpoolGRU, maxpoolGRU])\n",
    "    x2 = Dense(512, activation='elu', kernel_initializer=lecun_normal(seed=SEED))(concatenate2)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "    \n",
    "    concatenate3 = concatenate([x1, x2],axis=1)\n",
    "    x3 = Dense(512, activation='elu', kernel_initializer=lecun_normal(seed=SEED))(concatenate3)\n",
    "    x3 = Dropout(0.2)(x3)\n",
    "    outp = Dense(30, activation=\"sigmoid\")(x3)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss=custom_loss,\n",
    "                  optimizer=Adam(lr=1e-4, ),\n",
    "                  metrics=[bce,logcosh])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gkf = GroupKFold(n_splits=5).split(X=train.question_body, groups=train.question_body)\n",
    "kf = MultilabelStratifiedKFold(n_splits = 5, random_state = SEED)\n",
    "all_predictions = []\n",
    "\n",
    "for ind, (tr, val) in enumerate(kf.split(X_train,y_train)):\n",
    "#for ind, (tr, val) in enumerate(gkf):\n",
    "    if ind < 6:\n",
    "        X_tr = X_train[tr]\n",
    "        y_tr = y_train[tr]\n",
    "        X_vl = X_train[val]\n",
    "        y_vl = y_train[val]\n",
    "        \n",
    "        model = create_RNN_CNN_model()\n",
    "        \n",
    "        filepath = \"USE+DisstilBERT+LSTM_fold \" + str(ind+1) + \" bestmodel.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
    "                                     save_best_only=True, save_weights_only=False, mode='auto')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                      patience=5, min_lr=1e-7, verbose=1)\n",
    "        early_stop = EarlyStopping(monitor='val_loss',min_delta=0,\n",
    "                                   patience=15,mode='auto')\n",
    "        \n",
    "        model.fit(\n",
    "            X_tr, y_tr, epochs=100, batch_size=4, validation_data=(X_vl, y_vl), verbose=True, \n",
    "            callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n",
    "                                           patience=10, model_name=u'best_USE_model_batch.h5'),\n",
    "                       reduce_lr,early_stop,checkpoint]\n",
    "        )\n",
    "        model.load_weights('best_USE_model_batch.h5')\n",
    "        all_predictions.append(model.predict(X_test))\n",
    "        \n",
    "        os.remove('best_USE_model_batch.h5')\n",
    "    \n",
    "\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_test_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = USE_test_preds.max() + 1\n",
    "USE_test_preds = USE_test_preds/max_val + 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_test_preds = USE_test_preds\n",
    "Final_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[targets] = Final_test_preds\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "033ce6d23a7a4e038a66560bb3eb625e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "11aa4cd3f4d44770ba69425c6af8cdd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_27dfad62a5214df59c2d858050fac50c",
        "IPY_MODEL_9966dcddd61d4a9eaa5c45aa4c308dc1"
       ],
       "layout": "IPY_MODEL_3dc84647434b45a79c68ca2e46418a17"
      }
     },
     "18abf3643db943a6b0b92b94d545dc76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "27dfad62a5214df59c2d858050fac50c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_18abf3643db943a6b0b92b94d545dc76",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cc825019dfff42d6bdc820c7180bde2b",
       "value": 0
      }
     },
     "3dc84647434b45a79c68ca2e46418a17": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7c3ff172f6a245ceb451b7e34fff0b31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9966dcddd61d4a9eaa5c45aa4c308dc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7c3ff172f6a245ceb451b7e34fff0b31",
       "placeholder": "​",
       "style": "IPY_MODEL_033ce6d23a7a4e038a66560bb3eb625e",
       "value": " 0/? [00:00&lt;?, ?it/s]"
      }
     },
     "cc825019dfff42d6bdc820c7180bde2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
