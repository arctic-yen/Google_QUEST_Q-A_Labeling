{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "import sys\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "from math import floor, ceil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#TF&K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, losses, models, callbacks\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.constraints import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "print(tf.__version__)\n",
    "\n",
    "import spacy\n",
    "from transformers import *\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "SEED = 69\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mispell_dict = {\"aren't\" : \"are not\",          \"can't\" : \"cannot\",\n",
    "                \"couldn't\" : \"could not\",      \"couldnt\" : \"could not\",\n",
    "                \"didn't\" : \"did not\",          \"doesn't\" : \"does not\",\n",
    "                \"doesnt\" : \"does not\",         \"don't\" : \"do not\",\n",
    "                \"hadn't\" : \"had not\",          \"hasn't\" : \"has not\",\n",
    "                \"haven't\" : \"have not\",        \"havent\" : \"have not\",\n",
    "                \"he'd\" : \"he would\",           \"he'll\" : \"he will\",\n",
    "                \"he's\" : \"he is\",              \"i'd\" : \"I would\",\n",
    "                \"i'd\" : \"I had\",               \"i'll\" : \"I will\",\n",
    "                \"i'm\" : \"I am\",                \"isn't\" : \"is not\",\n",
    "                \"it's\" : \"it is\",              \"it'll\":\"it will\",\n",
    "                \"i've\" : \"I have\",             \"let's\" : \"let us\",\n",
    "                \"mightn't\" : \"might not\",      \"mustn't\" : \"must not\",\n",
    "                \"shan't\" : \"shall not\",        \"she'd\" : \"she would\",\n",
    "                \"she'll\" : \"she will\",         \"she's\" : \"she is\",\n",
    "                \"shouldn't\" : \"should not\",    \"shouldnt\" : \"should not\",\n",
    "                \"that's\" : \"that is\",          \"thats\" : \"that is\",\n",
    "                \"there's\" : \"there is\",        \"theres\" : \"there is\",\n",
    "                \"they'd\" : \"they would\",       \"they'll\" : \"they will\",\n",
    "                \"they're\" : \"they are\",        \"theyre\":  \"they are\",\n",
    "                \"they've\" : \"they have\",       \"we'd\" : \"we would\",\n",
    "                \"we're\" : \"we are\",            \"weren't\" : \"were not\",\n",
    "                \"we've\" : \"we have\",           \"what'll\" : \"what will\",\n",
    "                \"what're\" : \"what are\",        \"what's\" : \"what is\",\n",
    "                \"what've\" : \"what have\",       \"where's\" : \"where is\",\n",
    "                \"who'd\" : \"who would\",         \"who'll\" : \"who will\",\n",
    "                \"who're\" : \"who are\",          \"who's\" : \"who is\",\n",
    "                \"who've\" : \"who have\",         \"won't\" : \"will not\",\n",
    "                \"wouldn't\" : \"would not\",      \"you'd\" : \"you would\",\n",
    "                \"you'll\" : \"you will\",         \"you're\" : \"you are\",\n",
    "                \"you've\" : \"you have\",         \"'re\": \" are\",\n",
    "                \"wasn't\": \"was not\",           \"we'll\":\" will\",\n",
    "                \"didn't\": \"did not\",           \"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "          '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "          '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "          '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n",
    "          '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑',\n",
    "          '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    "          '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫',\n",
    "          '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    "          '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・',\n",
    "          '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x).replace(\"\\n\",\"\")\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def cln_space(x): \n",
    "    return \" \".join(x.split())\n",
    "\n",
    "def preprocess(x):\n",
    "    x= clean_text(x.lower())\n",
    "    x= replace_typical_misspell(x)\n",
    "    x= cln_space(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'roberta-base-tf_model.h5'\n",
    "PATH = '../input/google-quest-challenge/'\n",
    "ROBERTA_PATH = '../input/roberta-base-tf2/'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_PATH, do_lower_case=True)\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "df_train = pd.read_csv(PATH+'train.csv')\n",
    "df_test = pd.read_csv(PATH+'test.csv')\n",
    "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)\n",
    "\n",
    "output_categories = list(df_train.columns[11:])\n",
    "input_categories = list(df_train.columns[[1,2,5]])\n",
    "print('\\noutput categories:\\n\\t', output_categories)\n",
    "print('\\ninput categories:\\n\\t', input_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['question_body'] = df_train['question_body'].progress_map(lambda q: preprocess(q))\n",
    "df_train['answer'] = df_train['answer'].progress_map(lambda q: preprocess(q))\n",
    "df_train['question_title'] = df_train['question_title'].progress_map(lambda q: preprocess(q))\n",
    "\n",
    "df_test['question_body'] = df_test['question_body'].progress_map(lambda q: preprocess(q))\n",
    "df_test['answer'] = df_test['answer'].progress_map(lambda q: preprocess(q))\n",
    "df_test['question_title'] = df_test['question_title'].progress_map(lambda q: preprocess(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1, str2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=length,\n",
    "            truncation_strategy=truncation_strategy)\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        #print(input_ids)\n",
    "        #print(input_masks)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        title + ' ' + question, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        answer, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    return [input_ids_q, input_masks_q, input_segments_q,\n",
    "            input_ids_a, input_masks_a, input_segments_a]\n",
    "\n",
    "\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "        \n",
    "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "            np.asarray(input_masks_q, dtype=np.int32), \n",
    "            np.asarray(input_segments_q, dtype=np.int32),\n",
    "            np.asarray(input_ids_a, dtype=np.int32), \n",
    "            np.asarray(input_masks_a, dtype=np.int32), \n",
    "            np.asarray(input_segments_a, dtype=np.int32)]\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])\n",
    "\n",
    "\n",
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)\n",
    "\n",
    "def compute_spearmanr(trues, preds):\n",
    "    rhos = []\n",
    "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
    "        rhos.append(spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
    "    return np.mean(rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    '''\n",
    "    control learning rate\n",
    "    :param epoch:\n",
    "    :return:\n",
    "    '''\n",
    "    if epoch < 2:\n",
    "        lr = 3e-5\n",
    "    else:\n",
    "        lr = 3e-6\n",
    "    return lr\n",
    "\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay, verbose=2)\n",
    "\n",
    "keys = [0, 0.333333333333333, 0.44444444444444444, 0.55555555555555555, 0.666666666666666666666, 0.777777777777777777, 0.888888888888888888888, 1]\n",
    "def most_near(n):\n",
    "    dis = 10\n",
    "    m = 0\n",
    "    for e in keys:\n",
    "        if abs(e - n) < dis:\n",
    "            dis = abs(e - n) \n",
    "            m = e\n",
    "    # if n < 0.1:\n",
    "    #     m = 0\n",
    "    # elif n > 0.98:\n",
    "    #     m = 1\n",
    "    # else:\n",
    "    #     m = n\n",
    "    return m\n",
    "\n",
    "def repare(test_predictions):\n",
    "    result = []\n",
    "    for each in test_predictions:\n",
    "        each = each.tolist()\n",
    "        for i,t in enumerate(each):\n",
    "            temp1 = most_near(t)\n",
    "            # temp = random.randint(0,100)\n",
    "            # if temp == 10:\n",
    "            #     if temp1<0.99:\n",
    "            #         each[i] = temp1 + 0.01\n",
    "            #     else:\n",
    "            #         each[i] = temp1 - 0.01   \n",
    "            # else:\n",
    "            each[i] = temp1\n",
    "        result.append(each)\n",
    "\n",
    "    result = np.asarray(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(tf.keras.callbacks.Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        #print('cuurent lr: {}\\n'.format(self.clr()))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print('cuurent lr: {}\\n'.format(self.clr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n",
    "\n",
    "        self.valid_inputs = valid_data[0]\n",
    "        self.valid_outputs = valid_data[1]\n",
    "        self.test_inputs = test_data\n",
    "        self.value = 0\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions = []\n",
    "        self.test_predictions = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions = self.model.predict(self.valid_inputs, batch_size=self.batch_size)\n",
    "        valid_predictions1 = repare(self.valid_predictions)\n",
    "        rho_val = compute_spearmanr(self.valid_outputs, self.valid_predictions)\n",
    "        print('\\nbefore repare: rho_val = {}'.format(rho_val))\n",
    "        rho_val1 = compute_spearmanr(self.valid_outputs, valid_predictions1)\n",
    "        print('after repare: rho_val = {}\\n'.format(rho_val1))\n",
    "        \n",
    "        if rho_val1 > self.value:\n",
    "            self.value = rho_val1\n",
    "            self.model.save_weights(f'/kaggle/working/Finetune-Roberta-{fold}.h5')\n",
    "            \n",
    "            self.test_predictions.append(self.model.predict(self.test_inputs, batch_size=self.batch_size))\n",
    "\n",
    "            if len(self.test_predictions) > 0 :\n",
    "                self.test_predictions.pop()\n",
    "            self.test_predictions.append(self.model.predict(self.test_inputs, batch_size=self.batch_size))\n",
    "\n",
    "def create_model():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    config = RobertaConfig().from_pretrained(ROBERTA_PATH+'config.json') # print(config) to see settings\n",
    "    config.output_hidden_states = True # Set to True to obtain hidden states\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "    \n",
    "    \"\"\" bert_model = TFRobertaModel.from_pretrained(\n",
    "        BERT_PATH+MODEL, config=config, from_tf=True) \"\"\"\n",
    "\n",
    "    bert_model = TFRobertaModel.from_pretrained(\n",
    "        ROBERTA_PATH, config=config)\n",
    "\n",
    "    #bert_model = TFRobertaModel.from_pretrained(BERT_PATH+MODEL)\n",
    "\n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    q_embedding, q_pooler_output, q_hidden_states = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)\n",
    "    a_embedding, a_pooler_output, a_hidden_states = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)\n",
    "\n",
    "    q_pooler_output = tf.reshape(q_pooler_output,(-1,1,768))\n",
    "    q_h12 = tf.reshape(q_hidden_states[-1][:,0],(-1,1,768))\n",
    "    q_h11 = tf.reshape(q_hidden_states[-2][:,0],(-1,1,768))\n",
    "    q_h10 = tf.reshape(q_hidden_states[-3][:,0],(-1,1,768))\n",
    "    #print(q_h10.shape)\n",
    "    #print(q_pooler_output.shape)\n",
    "    q_concat_hidden = tf.keras.layers.Concatenate(axis=2)([q_pooler_output, q_h12, q_h11, q_h10])\n",
    "    q_x = tf.keras.layers.GlobalMaxPooling1D()(q_concat_hidden)\n",
    "\n",
    "    a_pooler_output = tf.reshape(a_pooler_output,(-1,1,768))\n",
    "    a_h12 = tf.reshape(a_hidden_states[-1][:,0],(-1,1,768))\n",
    "    a_h11 = tf.reshape(a_hidden_states[-2][:,0],(-1,1,768))\n",
    "    a_h10 = tf.reshape(a_hidden_states[-3][:,0],(-1,1,768))\n",
    "    #print(a_h10.shape)\n",
    "    #print(a_pooler_output.shape)\n",
    "    a_concat_hidden = tf.keras.layers.Concatenate(axis=2)([a_pooler_output, a_h12, a_h11, a_h10])\n",
    "    a_x = tf.keras.layers.GlobalMaxPooling1D()(a_concat_hidden)\n",
    "\n",
    "    concatenate_qa = tf.keras.layers.Concatenate()([q_x, a_x])\n",
    "    x = tf.keras.layers.Dropout(0.2)(concatenate_qa)\n",
    "    out = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=out)\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(),\n",
    "        loss=['binary_crossentropy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds = []\n",
    "test_preds = []\n",
    "valss = []\n",
    "\n",
    "clr = CyclicLR(base_lr=1e-5, max_lr=3e-5,\n",
    "               step_size=4863//2)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    \n",
    "    if fold < 3:\n",
    "        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "        train_outputs = outputs[train_idx]\n",
    "    \n",
    "        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "        valid_outputs = outputs[valid_idx]\n",
    "    \n",
    "        custom_callback = CustomCallback(\n",
    "            valid_data=(valid_inputs, valid_outputs), \n",
    "            test_data=test_inputs,\n",
    "            batch_size=16,\n",
    "            fold=fold)\n",
    "        \n",
    "        K.clear_session()\n",
    "        model = create_model()\n",
    "        model.fit(train_inputs, train_outputs, epochs=3, batch_size=8, callbacks=[custom_callback, lrate])\n",
    "        #model.save_weights(f'/kaggle/working/Finetune-Roberta-{fold}.h5')\n",
    "        valid_preds.append(model.predict(valid_inputs))\n",
    "        test_preds.append(model.predict(test_inputs))\n",
    "        \n",
    "        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n",
    "        print('validation score = ', rho_val)\n",
    "        valss.append(rho_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.iloc[:, 1:] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\n",
    "df_sub.to_csv('submission.csv', index=False)\n",
    "print('local cv = {}'.format(np.array(valss).mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03d22e2121ed46089fa01a8f5eafd6cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e9329dd5c4345498d7d2025194ef525": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "21650f20f11b4afb94c6cf62e91530da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_03d22e2121ed46089fa01a8f5eafd6cb",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fec823980a3a4aecba802110d2a76f41",
       "value": 0
      }
     },
     "2a7fa646373844779d27aa8c312f0e6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6fa8253c59b448209e4964ea356a326f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_21650f20f11b4afb94c6cf62e91530da",
        "IPY_MODEL_86b77d980a1d42479e3823d23b809d07"
       ],
       "layout": "IPY_MODEL_a824db755a64476fa3e287489d993089"
      }
     },
     "86b77d980a1d42479e3823d23b809d07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2a7fa646373844779d27aa8c312f0e6a",
       "placeholder": "​",
       "style": "IPY_MODEL_1e9329dd5c4345498d7d2025194ef525",
       "value": " 0/? [00:00&lt;?, ?it/s]"
      }
     },
     "a824db755a64476fa3e287489d993089": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fec823980a3a4aecba802110d2a76f41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
